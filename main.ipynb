{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f79937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahmac/Desktop/uni/kandidat/9_semester/ATIA/project/code/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/noahmac/Desktop/uni/kandidat/9_semester/ATIA/project/code/.venv/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/Users/noahmac/Desktop/uni/kandidat/9_semester/ATIA/project/code/.venv/lib/python3.11/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/Users/noahmac/Desktop/uni/kandidat/9_semester/ATIA/project/code/.venv/lib/python3.11/site-packages/mobile_sam/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/Users/noahmac/Desktop/uni/kandidat/9_semester/ATIA/project/code/.venv/lib/python3.11/site-packages/mobile_sam/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/Users/noahmac/Desktop/uni/kandidat/9_semester/ATIA/project/code/.venv/lib/python3.11/site-packages/mobile_sam/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/Users/noahmac/Desktop/uni/kandidat/9_semester/ATIA/project/code/.venv/lib/python3.11/site-packages/mobile_sam/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/Users/noahmac/Desktop/uni/kandidat/9_semester/ATIA/project/code/.venv/lib/python3.11/site-packages/mobile_sam/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    }
   ],
   "source": [
    "from mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "from mobile_sam import SamAutomaticMaskGenerator\n",
    "import torch\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d8d6189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR SCALES: [0.08589934592000005, 0.10737418240000006, 0.13421772800000006, 0.1677721600000001, 0.20971520000000007, 0.2621440000000001, 0.3276800000000001, 0.4096000000000001, 0.5120000000000001, 0.6400000000000001, 0.8, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): TinyViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (seq): Sequential(\n",
       "        (0): Conv2d_BN(\n",
       "          (c): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Conv2d_BN(\n",
       "          (c): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ConvLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x MBConv(\n",
       "            (conv1): Conv2d_BN(\n",
       "              (c): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (act1): GELU(approximate='none')\n",
       "            (conv2): Conv2d_BN(\n",
       "              (c): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (act2): GELU(approximate='none')\n",
       "            (conv3): Conv2d_BN(\n",
       "              (c): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (act3): GELU(approximate='none')\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (act): GELU(approximate='none')\n",
       "          (conv1): Conv2d_BN(\n",
       "            (c): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d_BN(\n",
       "            (c): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d_BN(\n",
       "            (c): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): BasicLayer(\n",
       "        dim=128, input_resolution=(128, 128), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x TinyViTBlock(\n",
       "            dim=128, input_resolution=(128, 128), num_heads=4, window_size=7, mlp_ratio=4.0\n",
       "            (drop_path): Identity()\n",
       "            (attn): Attention(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (local_conv): Conv2d_BN(\n",
       "              (c): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (act): GELU(approximate='none')\n",
       "          (conv1): Conv2d_BN(\n",
       "            (c): Conv2d(128, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d_BN(\n",
       "            (c): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=160, bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d_BN(\n",
       "            (c): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): BasicLayer(\n",
       "        dim=160, input_resolution=(64, 64), depth=6\n",
       "        (blocks): ModuleList(\n",
       "          (0-5): 6 x TinyViTBlock(\n",
       "            dim=160, input_resolution=(64, 64), num_heads=5, window_size=14, mlp_ratio=4.0\n",
       "            (drop_path): Identity()\n",
       "            (attn): Attention(\n",
       "              (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "              (qkv): Linear(in_features=160, out_features=480, bias=True)\n",
       "              (proj): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=160, out_features=640, bias=True)\n",
       "              (fc2): Linear(in_features=640, out_features=160, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (local_conv): Conv2d_BN(\n",
       "              (c): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
       "              (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (act): GELU(approximate='none')\n",
       "          (conv1): Conv2d_BN(\n",
       "            (c): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d_BN(\n",
       "            (c): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d_BN(\n",
       "            (c): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): BasicLayer(\n",
       "        dim=320, input_resolution=(64, 64), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x TinyViTBlock(\n",
       "            dim=320, input_resolution=(64, 64), num_heads=10, window_size=7, mlp_ratio=4.0\n",
       "            (drop_path): Identity()\n",
       "            (attn): Attention(\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (qkv): Linear(in_features=320, out_features=960, bias=True)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (local_conv): Conv2d_BN(\n",
       "              (c): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_head): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=320, out_features=1000, bias=True)\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = \"vit_t\"\n",
    "sam_checkpoint = \"./MobileSam/weights/mobile_sam.pt\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "\n",
    "mobile_sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "mobile_sam.to(device=device)\n",
    "mobile_sam.eval()\n",
    "\n",
    "# predictor = SamPredictor(mobile_sam)\n",
    "# predictor.set_image(<your_image>)\n",
    "# masks, _, _ = predictor.predict(<input_prompts>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbb624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='./data/COD10K-v3/Train/Image/COD10K-CAM-1-Aquatic-1-BatFish-1.jpg' mode='r' encoding='UTF-8'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = open(\"./data/COD10K-v3/Train/Image/COD10K-CAM-1-Aquatic-1-BatFish-1.jpg\")\n",
    "\n",
    "cond_img_e = gr.Image(label=\"Input\", value=default_example[0], type=\"pil\")\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c137a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m mask_generator = SamAutomaticMaskGenerator(mobile_sam)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m masks = \u001b[43mmask_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/uni/kandidat/9_semester/ATIA/project/code/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/uni/kandidat/9_semester/ATIA/project/code/.venv/lib/python3.11/site-packages/mobile_sam/automatic_mask_generator.py:163\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator.generate\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[33;03mGenerates masks for the given image.\u001b[39;00m\n\u001b[32m    140\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m \u001b[33;03m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# Generate masks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m mask_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.min_mask_region_area > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/uni/kandidat/9_semester/ATIA/project/code/.venv/lib/python3.11/site-packages/mobile_sam/automatic_mask_generator.py:198\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator._generate_masks\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np.ndarray) -> MaskData:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     orig_size = \u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m[:\u001b[32m2\u001b[39m]\n\u001b[32m    199\u001b[39m     crop_boxes, layer_idxs = generate_crop_boxes(\n\u001b[32m    200\u001b[39m         orig_size, \u001b[38;5;28mself\u001b[39m.crop_n_layers, \u001b[38;5;28mself\u001b[39m.crop_overlap_ratio\n\u001b[32m    201\u001b[39m     )\n\u001b[32m    203\u001b[39m     \u001b[38;5;66;03m# Iterate over image crops\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: '_io.TextIOWrapper' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "mask_generator = SamAutomaticMaskGenerator(mobile_sam)\n",
    "masks = mask_generator.generate(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
